{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10e97cc-59d8-4c33-a41d-8898646b28c9",
   "metadata": {},
   "source": [
    "# Sequential Multi-Task Fine-tuing: BERT Fine-Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook fine-tunes a BERT model for the given dataset. One approach is to use a pre-trained model and fine-tune it directly on the given dataset. However, there are other fine-tuning approaches, such as Multi-Task fine-tuning [2]. In this notebook, we adopt the Multi-Task fine-tuning approach. Specifically, we use a pre-trained BERT model and fine-tune it on a target domain — in our case, a dataset named Yahoo! Answers. This task involves question classification, similar to the exercise's task. Next, we utilize the fine-tuned model from the previous step and further fine-tune it on the main target dataset, which is the StackOverflow dataset.\n",
    "\n",
    "Before going through the implementations, we explain the reason for the packages we imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771fd802-33c7-4ccb-b57c-f3819899de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device is  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # for reading the csv files.\n",
    "from sklearn.model_selection import train_test_split # to split the validation set into two sets named validation and test.\n",
    "#from sklearn.preprocessing import LabelEncoder # To encode labels into unuqie values.\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader # To create dataset and loaders from train, validation, and test dataframes\n",
    "'''\n",
    "The Following imports are:\n",
    "1- BertTokenizer is a tokenizer class specifically designed for BERT models. \n",
    "It handles the conversion of text into tokens, adding special tokens required by BERT, \n",
    "and creating input tensors needed for the model.\n",
    "2- BertForSequenceClassification\n",
    "3- AdamW: An extension of Adam optimizer with weight decay.\n",
    "4- get_linear_schedule_with_warmup: we can set up a scheduler which warms up for num_warmup_steps and then linearly decays \n",
    "to 0 by the end of training.\n",
    "'''\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "#from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# We check whether cuda is available or not.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Available device is ', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47472c-b845-49ef-a8a8-913a41b2a0c6",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58db2312-6dbd-4ca0-b7ab-2da0741f1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        #Utilizing the tokenzier specific for BERT\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, # Add special tokens to the inputs of the BERT\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True, # The text is truncated if it exceeds the max length\n",
    "            return_attention_mask=True, # a binary mask indicating which tokens are actual input tokens and which are padding tokens\n",
    "            return_tensors='pt', # Return results with Tensor type\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initializing the tokenizer specific for pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Creating data loaders\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size,target_label):\n",
    "    dataset = TextDataset(\n",
    "        texts=df['Text'].to_numpy(),\n",
    "        labels=df[target_label].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8318c-1a70-4fc1-9558-3fa8c5abf86b",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "In the following cell, we\n",
    "\n",
    "1- creat the model\n",
    "\n",
    "2- Creat the optimizer and adjust the learning rate scheduling. The hyperparameter seeting have been don based on the following paper [2]. \n",
    "\n",
    "Sun, Chi, et al. \"How to fine-tune bert for text classification?.\" Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18–20, 2019, proceedings 18. Springer International Publishing, 2019.\n",
    "https://arxiv.org/pdf/1905.05583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e47a21-0ce1-42f7-af91-3750af9a9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the model, we use pre-trained BERT speficied by 'bert-base-uncased'. We just change the head of the\n",
    "# the model to produce three output in accordance with the number of classes in our datasets.\n",
    "class BERTModel(object):\n",
    "    def __init__(self,model_dir=None,num_classes=None):\n",
    "        print('model_dir',model_dir)\n",
    "        if model_dir is None:\n",
    "            self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)  # num_classes is the number of classes\n",
    "            # Move the model to GPU if available            \n",
    "            self.model = self.model.to(device)\n",
    "            \n",
    "            # In this experiment, we freez the encoder part. To put it diffeently, we just train the head of the model. \n",
    "            # We can also fine-tune the whole model. However, since the model is fine-tuned and inorder to speed up fine-tuning we freez the base\n",
    "            # of the model which is the encoder part.\n",
    "            #for param in model.base_model.parameters():\n",
    "             #   param.requires_grad = False\n",
    "        else:\n",
    "            self.model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "            self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_classes)\n",
    "            self.model = self.model.to(device)\n",
    "            \n",
    "        # Define the optimizer and the learning rate scheduler\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, weight_decay=0.95)\n",
    "        total_steps = len(train_data_loader) * num_classes  # num_classes is the number of epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d62b6c-75a3-4aad-a6bb-80196a1e1051",
   "metadata": {},
   "source": [
    "### Train and Validation procedure\n",
    "\n",
    "Here, we implement the common train and validation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bd20d0-084f-4c89-a623-f601d9244e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        #loss = outputs.loss # Computed based on biult in loss function. In this case crossentropy loss function\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses).item()\n",
    "\n",
    "# Validation function\n",
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            labels = d['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f0f94-7587-49c0-83fb-906bcc754feb",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "This cells implements the training loop. Epochs specify the number of epochs the model sees the data.\n",
    "\n",
    "We also collect statistics such as loss and accuracy for each of train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a3adf7-1b48-44eb-9d6c-5d2e1bb2ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_Training(EPOCHS,dataset_name,model,optimizer,scheduler):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "        print('-' * 10)\n",
    "    \n",
    "        train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device, scheduler)\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "    \n",
    "        val_acc, val_loss = eval_model(model, val_data_loader, device)\n",
    "        print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        test_acc, test_loss = eval_model(model, test_data_loader, device)\n",
    "        print(f'Test   loss {test_loss} accuracy {test_acc}')\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        print()\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained('./saved_model/'+dataset_name)\n",
    "    print('model_saved')\n",
    "    return (train_losses,train_accs,val_losses,val_accs,test_losses,test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de5792-5456-47fa-9629-57860e04c1b7",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f76063-1f1b-46b2-9b56-9a8064aa495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : stackoverflow\n",
      "train data:  (45000, 2)  val data:  (12000, 2)  test data:  (3000, 2)\n",
      "model_dir ./saved_model/YahoAnswers/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain data: \u001b[39m\u001b[38;5;124m'\u001b[39m,train_df\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m val data: \u001b[39m\u001b[38;5;124m'\u001b[39m,val_df\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m test data: \u001b[39m\u001b[38;5;124m'\u001b[39m,test_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstackoverflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mBERTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./saved_model/YahoAnswers/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYahoAnswers\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     35\u001b[0m     model \u001b[38;5;241m=\u001b[39m BERTModel(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mBERTModel.__init__\u001b[0;34m(self, model_dir, num_classes)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# In this experiment, we freez the encoder part. To put it diffeently, we just train the head of the model. \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# We can also fine-tune the whole model. However, since the model is fine-tuned and inorder to speed up fine-tuning we freez the base\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# of the model which is the encoder part.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#for param in model.base_model.parameters():\u001b[39;00m\n\u001b[1;32m     15\u001b[0m      \u001b[38;5;66;03m#   param.requires_grad = False\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dir)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define the optimizer and the learning rate scheduler\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_labels'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "EPOCHS = 1\n",
    "MAX_LEN = 128 # Maximum length of sequence.\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_name = 'stackoverflow'\n",
    "\n",
    "if dataset_name == 'stackoverflow':\n",
    "    # Loading train and validation sets, those that we proprocessed.\n",
    "    train_df = pd.read_csv(\"preprocessed_train.csv\")\n",
    "    val_df = pd.read_csv(\"preprocessed_val.csv\")\n",
    "    num_classes = train_df['Y'].nunique()\n",
    "    target_label = 'Y'\n",
    "elif dataset_name == 'YahoAnswers':\n",
    "    train_df = pd.read_csv(\"./yahoo_answers_csv/preprocessed_train.csv\")\n",
    "    train_df = train_df\n",
    "    val_df = pd.read_csv(\"./yahoo_answers_csv/preprocessed_test.csv\")\n",
    "    num_classes = train_df['label'].nunique()\n",
    "    target_label = 'label'\n",
    "    print('number of classes ',num_classes)\n",
    "print('Dataset :' ,dataset_name)\n",
    "# We split validation set into two sets validation and test sets\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.2, random_state=42, stratify=val_df[target_label])\n",
    "# Creating data loaders from each train. validation, and test sets.\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE,target_label)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE,target_label)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE,target_label)\n",
    "# We print the shapes, indicating the size and number of samples in each of train, validation, and test sets.\n",
    "print('train data: ',train_df.shape,' val data: ',val_df.shape,' test data: ',test_df.shape)\n",
    "\n",
    "\n",
    "if dataset_name == 'stackoverflow':\n",
    "    model = BERTModel(model_dir='./saved_model/YahoAnswers/',num_classes=num_classes)\n",
    "elif dataset_name == 'YahoAnswers':\n",
    "    model = BERTModel(num_classes=num_classes)\n",
    "\n",
    "Model_Training(EPOCHS,dataset_name,model.model,model.optimizer,model.scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578f2d3-49bd-438f-b807-74d7902a6f4a",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1] https://huggingface.co/transformers/v3.5.1/training.html\n",
    "\n",
    "[2] Sun, Chi, et al. \"How to fine-tune bert for text classification?.\" Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18–20, 2019, proceedings 18. Springer International Publishing, 2019.\n",
    "https://arxiv.org/pdf/1905.05583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735f895-dab3-4d04-a971-14c31990c45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cluepoints)",
   "language": "python",
   "name": "cluepoints"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
