{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa022cc-8aa9-4642-9f98-7445dfc52d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db3975-dba6-4711-860b-1445fbc3b922",
   "metadata": {},
   "source": [
    "# Creating Dataset class\n",
    "1- We read the pre-processed dataset. \n",
    "\n",
    "2- We implement minmax_normalization to apply normalization on each fold of training and validation.\n",
    "\n",
    "3- HospitalDataset: A custome dataset class to create a data loader from.\n",
    "\n",
    "4- Finally, since dataset is extremly imblance we follow stratified KFold. As a result at the end we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef96b8ff-b10e-4270-bc66-97db1be717cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_hospital_stay_data.csv\")\n",
    "\n",
    "num_classes = df['Stay'].nunique()\n",
    "X_columns = df.columns[2:-1]\n",
    "y_columns = df.columns[-1]\n",
    "data = df[X_columns].to_numpy()\n",
    "labels = df[y_columns].to_numpy()\n",
    "\n",
    "data, labels = shuffle(data, labels,random_state=42)\n",
    "\n",
    "        \n",
    "def minmax_normalization(input_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(input_features)\n",
    "    input_features_scaled = scaler.transform(input_features)\n",
    "    return input_features_scaled\n",
    "\n",
    "class HospitalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels,transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label\n",
    "\n",
    "batch_size = 32\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f5611-4072-4ef2-8a40-11772ff6c201",
   "metadata": {},
   "source": [
    "### Defining Focal loss to cope with data imbalacne issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf3326-49c4-4e31-aedd-814dbbc2c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(My_ResNet50, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Here we replace the first conv layer whith another one that has one input channel\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=3, bias=False)\n",
    "        #resnet.bn1 = nn.BatchNorm1d(64)\n",
    "        #resnet.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # We replace fully connected layer to match number of classes\n",
    "        resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "        self.resnet = resnet\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).unsqueeze(3).float()\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fae44c-e3ff-429e-89d1-b269666999cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1D():\n",
    "    def __init__(self):\n",
    "        filters_conv = 64\n",
    "        filters_dense = 1536\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Conv1d(1, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Conv1d(filters_conv, filters_conv, 3, padding=1),\n",
    "        nn.BatchNorm1d(filters_conv),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(2),\n",
    "    \n",
    "        nn.Flatten(),\n",
    "    \n",
    "        nn.Linear(filters_dense * 4, filters_dense, bias=False),\n",
    "        nn.BatchNorm1d(filters_dense),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Linear(filters_dense, filters_dense, bias=False),\n",
    "        nn.BatchNorm1d(filters_dense),\n",
    "        nn.ReLU(),\n",
    "    \n",
    "        nn.Linear(filters_dense, 11, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb182939-14d9-412d-8c4d-5f1b06d6ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=7,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81cf2f0-16a0-46f3-9352-fdbf093eb714",
   "metadata": {},
   "source": [
    "### Training and valdidation fucntions\n",
    "\n",
    "Since dataset is imbalance we utilize PR_AUC metric for our evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81b4e64-d198-42f8-bf58-1da82483ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=10):\n",
    "    print('start training')\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        print(epoch)\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.to(device).unsqueeze(1).float())\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss.append(running_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "    return model, train_loss\n",
    "    \n",
    "def validate_model(model,val_loader,criterion):\n",
    "    model.eval()\n",
    "    val_labels_list = []\n",
    "    val_probs_list = []\n",
    "    val_loss = None\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_labels_list.extend(labels.cpu().numpy())\n",
    "            val_probs_list.extend(probs.cpu().numpy())\n",
    "        val_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    val_labels_array = np.array(val_labels_list)\n",
    "    val_probs_array = np.array(val_probs_list)\n",
    "\n",
    "    return val_probs_array, val_labels_array, val_loss\n",
    "\n",
    "def compute_multiclass_pr_auc(y_true, y_pred_probs, num_classes):\n",
    "    pr_aucs = []\n",
    "    for i in range(num_classes):\n",
    "        # Binarize the true labels for the current class\n",
    "        y_true_bin = (y_true == i).astype(int)\n",
    "        \n",
    "        # Compute precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_pred_probs[:, i])\n",
    "        \n",
    "        # Compute PR-AUC\n",
    "        pr_auc = auc(recall, precision)\n",
    "        pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Macro-average PR-AUC\n",
    "    macro_pr_auc = np.mean(pr_aucs)\n",
    "    return pr_aucs, macro_pr_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd051c-7292-4c5f-8307-fa9967d89cb0",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We utilize 5fold cross validation training startegy. For each given fold, we train the model for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3684a985-0b3c-4de9-9653-c7a1c507ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "traing shape  (251034, 48)\n",
      "val shape (62759, 48)\n",
      "start training\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x1536 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Training the model on the given fold\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m model,train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m models_list\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     49\u001b[0m train_fold_loss\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(train_loss))\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cluepoints/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1536 and 512x128)"
     ]
    }
   ],
   "source": [
    "models_list = []\n",
    "all_pr_aucs = []\n",
    "all_macro_pr_aucs = []\n",
    "train_fold_loss = []\n",
    "val_fold_loss = []\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data, labels)):\n",
    "    print(f\"Fold {fold_idx + 1}/{k_folds}\")\n",
    "\n",
    "    # Splitting data into train and validation sets\n",
    "    '''\n",
    "    train_data = torch.from_numpy(data[train_index])\n",
    "    val_data = torch.from_numpy(data[val_index])\n",
    "    train_labels = torch.from_numpy(labels[train_index])\n",
    "    val_labels = torch.from_numpy(labels[val_index])\n",
    "    '''\n",
    "    train_data = data[train_index]\n",
    "    print('traing shape ',train_data.shape)\n",
    "    val_data = data[val_index]\n",
    "    print('val shape',val_data.shape)\n",
    "    train_labels = labels[train_index]\n",
    "    val_labels = labels[val_index]\n",
    "\n",
    "    # Calculate mean and std for the training set of the current fold\n",
    "    train_data = minmax_normalization(train_data)\n",
    "    val_data = minmax_normalization(val_data)\n",
    "\n",
    "\n",
    "    # Create datasets with transformations\n",
    "    train_dataset = HospitalDataset(train_data, train_labels, transform=None)\n",
    "    val_dataset = HospitalDataset(val_data, val_labels, transform=None)\n",
    "\n",
    "    # Creating data loaders for training and validation\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initializing ResNet-50 model\n",
    "    model = VGG_1D().model\n",
    "    #model = My_ResNet50(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining loss function and optimizer\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = FocalLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # Training the model on the given fold\n",
    "    model,train_loss = train_model(model, train_loader, optimizer, criterion, epochs=5)\n",
    "    models_list.append(model)\n",
    "    train_fold_loss.append(np.mean(train_loss))\n",
    "\n",
    "    # Validating the model on the given fold\n",
    "    val_probs, val_labels, val_loss = validate_model(model,val_loader, criterion)\n",
    "    val_fold_loss.append(val_loss)\n",
    "\n",
    "    # Computing PR-AUC for this fold\n",
    "    pr_aucs, macro_pr_auc = compute_multiclass_pr_auc(val_labels, val_probs, num_classes)\n",
    "    all_pr_aucs.append(pr_aucs)\n",
    "    all_macro_pr_aucs.append(macro_pr_auc)\n",
    "\n",
    "    print(f\"Fold {fold_idx + 1} PR-AUC: {pr_aucs}\")\n",
    "    print(f\"Fold {fold_idx + 1} Macro-Averaged PR-AUC: {macro_pr_auc}\")\n",
    "    \n",
    " # After the loop, you can summarize the results\n",
    "mean_macro_pr_auc = np.mean(all_macro_pr_aucs)\n",
    "std_macro_pr_auc = np.std(all_macro_pr_aucs)\n",
    "\n",
    "print(f\"Mean Macro-Averaged PR-AUC: {mean_macro_pr_auc}\")\n",
    "print(f\"Standard Deviation of Macro-Averaged PR-AUC: {std_macro_pr_auc}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa900fa-6c60-4842-a6a0-5fe392d446ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cluepoints)",
   "language": "python",
   "name": "cluepoints"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
