{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc70bc8-043c-48ee-b335-bff555d806dc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook trains a machine learning model named XGBoost with decision trees as the base model on the given datasets.\n",
    "\n",
    "Before going through the implementation of the model, we import neccessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a8f61-f231-4795-9ba5-8cdcdd026d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd # For reading the csv files.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold # for kfold cross validation, will be explained in details later\n",
    "import xgboost as xgb # Our selection for machine learning model. will be explained more later\n",
    "#Following we import some metrics for evaluating the model. The selection reason will be explained\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score, roc_curve\n",
    "from sklearn import metrics\n",
    "import tqdm\n",
    "# To train the models on each fold in paralle.\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# We read the datsets.\n",
    "df = pd.read_csv(\"preprocessed_hospital_stay_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc628f40-4cac-4fcd-b9da-2bfe58c89d91",
   "metadata": {},
   "source": [
    "In the following cell, we specify number of classes, input features and their target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1f81f-9c82-4273-a954-784dbcc759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify the number of clases by counting the number of uniqie values\n",
    "class_num = df['Stay'].nunique()\n",
    "# The first two coulmns are not considered as inputs as they are case id and patient id. \n",
    "X_columns = df.columns[2:-1]\n",
    "# last column as the label for each sample\n",
    "y_columns = df.columns[-1]\n",
    "# convert them to numpy array\n",
    "X = df[X_columns].to_numpy()\n",
    "Y = df[y_columns].to_numpy()\n",
    "# We shuffle the data.\n",
    "X, Y = shuffle(X, Y,random_state=42)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451d8fa-873a-4ba3-9dbc-c9588642d583",
   "metadata": {},
   "source": [
    "# Confige defition\n",
    "\n",
    "In this cell, we set the hyperparameter of the model. This paramters can be also tuned in a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbad7da-8936-455a-bf7e-5a27bc5e6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "configes={'model_1':{\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': class_num,  # Number of classes in the dataset\n",
    "    'eval_metric': 'merror',  # Evaluation metric (multi-class classification error rate)\n",
    "    'max_depth': 20,  # Depth of each tree\n",
    "    'learning_rate': 0.01,  # Learning rate, controls the boosting process\n",
    "    'n_estimators': 500  # Number of boosted trees to fit\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017a465-8e02-4810-9c34-6d77099712a2",
   "metadata": {},
   "source": [
    "# Model Selection, Training loop, and validation metrics\n",
    "\n",
    "1- Model Selection: XGBoost with decision tree. Since the datset is hugely imblance we consider XGB with Decsion tree, since decsion trees can handle imblanced data.\n",
    "\n",
    "2- We utilize stratified Kfold instead of usall kfold cross validation. The stratified version make sure that each samples in each fold follows the same destribution as that of the full data. This is suitable when we face with data imblance\n",
    "\n",
    "3- We utilzie different metrics, specially PR-AUC when we deal with imblance data and we want to measure the confiedence of the model in making the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718913aa-e539-48b4-8ab5-b1a00aaa4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We have defined a minmax normalization to normalize the input features. This function is uesed in case that\n",
    "we use other machine learning models such as logistic regression. In case of XGBoost with decion tree as the based model\n",
    "we don't need to do minmax normalization since decision tree considers reletive oredering of features for creating the tree\n",
    "not the absolute values of the trees.\n",
    "'''\n",
    "def minmax_normalization(input_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(input_features)\n",
    "    input_features_scaled = scaler.transform(input_features)\n",
    "    return input_features_scaled\n",
    "\n",
    "#This function trains a model on the given fold.\n",
    "# To train the model on each fold, we follow mini-batch training strategy\n",
    "def train_single_fold(train_index, test_index, params, X, y, batch_size):\n",
    "    '''\n",
    "    inputs:\n",
    "            train_index: index of the traing samples for the current fold traing\n",
    "            test_index: index of the test samples for the current fold training\n",
    "            X: Input features of the all samples\n",
    "            y: output label of the all samples\n",
    "            batch_size: size of the batch for mini batch training\n",
    "    output:\n",
    "            model\n",
    "            A set of evaluation metrics\n",
    "    '''\n",
    "    print('Runing seperate process for training on single fold')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Computing number of iteration based on the defined batch size and number of samples in the given fold.\n",
    "    num_batches = len(X_train) // batch_size\n",
    "    model = None\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "\n",
    "        if batch_end > len(X_train):\n",
    "            batch_end = len(X_train)\n",
    "\n",
    "        X_batch = X_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_batch, label=y_batch)\n",
    "\n",
    "        if model is None:\n",
    "            model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtrain, 'train')], \n",
    "                              early_stopping_rounds=10, verbose_eval=False)\n",
    "        else:\n",
    "            model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtrain, 'train')], \n",
    "                              early_stopping_rounds=10, verbose_eval=False, xgb_model=model)\n",
    "    #Perfomes prediction on unse\n",
    "    fold_predictions = model.predict(xgb.DMatrix(X_test))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, fold_predictions, pos_label=class_num)\n",
    "    pr_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    return model, pr_auc\n",
    "'''\n",
    "# This function implements the training loop.  \n",
    "'''\n",
    "def training(params, k, X, y, batch_size):\n",
    "    '''\n",
    "    Inputs:\n",
    "            params: paramters of the machine learning model\n",
    "            k: number of folds\n",
    "            x: input features\n",
    "            y: output label\n",
    "            batch_size: size of the batch\n",
    "    \n",
    "    Outputs:\n",
    "        model: the trained model\n",
    "        A touple of statistics\n",
    "    '''\n",
    "    accuracy_scores = []\n",
    "    precision_macro_scores = []\n",
    "    recall_macro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    pr_auc_scores = []\n",
    "    models = []\n",
    "    # Defines startified Kfold\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    #In order to train the models in each fold in parallel we use ProcessPoolExecutor\n",
    "    futures = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            futures.append(executor.submit(train_single_fold, train_index, test_index, params, X, y, batch_size))\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "            model, pr_auc = future.result()\n",
    "            models.append(model)\n",
    "            pr_auc_scores.append(pr_auc)\n",
    "            \n",
    "    return models, pr_auc_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d369d-e774-4b38-9a51-d702c86922b0",
   "metadata": {},
   "source": [
    "The following cell, start traing phase for given parameters and number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522a0ab-a3f3-4ec6-b08b-c5efd674a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "num_folds = [10]\n",
    "batch_size = 50\n",
    "for folds in num_folds:\n",
    "  results[folds] = {}\n",
    "  for model_name in configes.keys():\n",
    "    models, mymetrics = training(configes[model_name],folds,X,Y,batch_size)\n",
    "    results[folds][model_name] = (models,mymetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385c735-91bf-4b2d-979f-19097b492ba8",
   "metadata": {},
   "source": [
    "# Plotting the AUC result per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031db792-0bcb-42d7-9890-654edcf4e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "markers = ['o']\n",
    "colors = ['red']\n",
    "for folds in num_folds:\n",
    "  for index,model_name in enumerate(results[folds].keys()):\n",
    "    print('\\n'+model_name+' with configes:')\n",
    "    if 'repeat' in model_name:\n",
    "      print(configes['model6'])\n",
    "    else:\n",
    "      print(configes[model_name])\n",
    "\n",
    "    metrics = results[folds][model_name][1]\n",
    "    x = np.arange(len(metrics[1]))\n",
    "\n",
    "    plt.plot(x, metrics[2],markers[index]+'-',color=colors[index])\n",
    "    plt.set_title('AUC over '+str(folds)+' folds')\n",
    "    plt.set_xlabel('Folds')\n",
    "    plt.set_ylabel('AUC')\n",
    "      \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cluepoints)",
   "language": "python",
   "name": "cluepoints"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
