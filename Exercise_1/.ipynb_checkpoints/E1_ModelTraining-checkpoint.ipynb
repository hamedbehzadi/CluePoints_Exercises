{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5a8f61-f231-4795-9ba5-8cdcdd026d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"preprocessed_hospital_stay_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b1f81f-9c82-4273-a954-784dbcc759f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313793, 48)\n",
      "(313793,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "class_num = df['Stay'].nunique()\n",
    "\n",
    "X_columns = df.columns[2:-1]\n",
    "y_columns = df.columns[-1]\n",
    "X = df[X_columns].to_numpy()\n",
    "Y = df[y_columns].to_numpy()\n",
    "\n",
    "X, Y = shuffle(X, Y,random_state=42)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbad7da-8936-455a-bf7e-5a27bc5e6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "configes={'model_1':{\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': class_num,  # Number of classes in the dataset\n",
    "    'eval_metric': 'merror',  # Evaluation metric (multi-class classification error rate)\n",
    "    'max_depth': 20,  # Depth of each tree\n",
    "    'learning_rate': 0.01,  # Learning rate, controls the boosting process\n",
    "    'n_estimators': 500  # Number of boosted trees to fit\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718913aa-e539-48b4-8ab5-b1a00aaa4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "import tqdm\n",
    "\n",
    "def minmax_normalization(input_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(input_features)\n",
    "    input_features_scaled = scaler.transform(input_features)\n",
    "    return input_features_scaled\n",
    "\n",
    "def compute_multiclass_pr_auc(y_true, y_pred_probs, num_classes):\n",
    "    pr_aucs = []\n",
    "    for i in range(num_classes):\n",
    "        # Binarize the true labels for the current class\n",
    "        y_true_bin = (y_true == i).astype(int)\n",
    "        \n",
    "        # Compute precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_pred_probs[:, i])\n",
    "        \n",
    "        # Compute PR-AUC\n",
    "        pr_auc = auc(recall, precision)\n",
    "        pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Macro-average PR-AUC\n",
    "    macro_pr_auc = np.mean(pr_aucs)\n",
    "    return pr_aucs, macro_pr_auc\n",
    "\n",
    "    \n",
    "def training(params,k,X,y,batch_size):\n",
    "    # Initializing lists to store evaluation metrics\n",
    "    accuracy_scores = []\n",
    "    precision_macro_scores = []\n",
    "    recall_macro_scores = []\n",
    "    f1_macro_scores = []\n",
    "    predictions = []\n",
    "    pr_auc_scores = []\n",
    "    \n",
    "    models = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=False)\n",
    "    \n",
    "    # Iterating over each fold\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X,Y)):\n",
    "        print('Fold ',str(index+1))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = minmax_normalization(X_train)\n",
    "        X_test = minmax_normalization(X_test)\n",
    "        \n",
    "        num_batches = len(X_train) // batch_size\n",
    "        print('number of baches',num_batches)\n",
    "        model = None\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "\n",
    "            # Handle the last batch which may be smaller\n",
    "            if batch_end > len(X_train):\n",
    "                batch_end = len(X_train)\n",
    "\n",
    "            X_batch = X_train[batch_start:batch_end]\n",
    "            y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "            # Convert data to DMatrix format for XGBoost\n",
    "            dtrain = xgb.DMatrix(X_batch, label=y_batch)\n",
    "\n",
    "           # Train the model on the current batch\n",
    "            if model is None:\n",
    "                # Initial training\n",
    "                model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtrain, 'train')], \n",
    "                                  early_stopping_rounds=10, verbose_eval=False)\n",
    "            else:\n",
    "                # Continuing training with existing model\n",
    "                model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtrain, 'train')], \n",
    "                                  early_stopping_rounds=10, verbose_eval=False, xgb_model=model)\n",
    "\n",
    "\n",
    "\n",
    "        models.append(model)\n",
    "        # Making predictions on the test data for this fold\n",
    "        fold_predictions = model.predict(X_test)\n",
    "\n",
    "        # Calculating evaluation metrics for this fold\n",
    "        accuracy_scores.append(accuracy_score(y_test, fold_predictions))\n",
    "        precision_macro_scores.append(precision_score(y_test, fold_predictions, average='macro'))\n",
    "        recall_macro_scores.append(recall_score(y_test, fold_predictions, average='macro'))\n",
    "        f1_macro_scores.append(f1_score(y_test, fold_predictions, average='macro'))\n",
    "        pr_auc_scores.append(average_precision_score(y_test, fold_predictions))\n",
    "        print(pr_auc_scores[-1])\n",
    "        \n",
    "\n",
    "    return models, (accuracy_scores,precision_macro_scores,recall_macro_scores,f1_macro_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4522a0ab-a3f3-4ec6-b08b-c5efd674a712",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m results[folds] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m configes\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 7\u001b[0m   models, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   results[folds][model_name] \u001b[38;5;241m=\u001b[39m (models,metrics)\n",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(params, k, X, y, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Iterating over each fold\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(X,Y)):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mstr\u001b[39m(\u001b[43mpr_auc_scores\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m))\n\u001b[1;32m     46\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X[train_index], X[test_index]\n\u001b[1;32m     47\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "num_folds = [10]\n",
    "batch_size = 300\n",
    "for folds in num_folds:\n",
    "  results[folds] = {}\n",
    "  for model_name in configes.keys():\n",
    "    models, metrics = training(configes[model_name],folds,X,Y,batch_size)\n",
    "    results[folds][model_name] = (models,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c5017-9e98-4182-87af-47143978bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "markers = ['o','s','d','x','^','v','p','*']\n",
    "colors = ['red','green','blue','purple','pink','orange','brown','black']\n",
    "for folds in num_folds:\n",
    "  fig1, axs1 = plt.subplots(2, 2, figsize=(10, 8))\n",
    "  for index,model_name in enumerate(results[folds].keys()):\n",
    "\n",
    "    print('\\n'+model_name+' with configes:')\n",
    "    if 'repeat' in model_name:\n",
    "      print(configes['model6'])\n",
    "    else:\n",
    "      print(configes[model_name])\n",
    "\n",
    "    metrics = results[folds][model_name][1]\n",
    "    x = np.arange(len(metrics[1]))\n",
    "    axs1[0,0].plot(x, metrics[1],markers[index]+'-',color=colors[index])\n",
    "    axs1[0,0].set_title('Precision Macro results over '+str(folds)+' folds')\n",
    "    axs1[0,0].set_xlabel('Folds')\n",
    "    axs1[0,0].set_ylabel('Precision Macro')\n",
    "\n",
    "    axs1[0,1].plot(x, metrics[2],markers[index]+'-',color=colors[index])\n",
    "    axs1[0,1].set_title('Recall Macro results over '+str(folds)+' folds')\n",
    "    axs1[0,1].set_xlabel('Folds')\n",
    "    axs1[0,1].set_ylabel('Recall Macro')\n",
    "\n",
    "    axs1[1,0].plot(x, metrics[3],markers[index]+'-',color=colors[index])\n",
    "    axs1[1,0].set_title('F1 Macro results over '+str(folds)+' folds')\n",
    "    axs1[1,0].set_xlabel('Folds')\n",
    "    axs1[1,0].set_ylabel('F1 Macro')\n",
    "\n",
    "    x = np.arange(len(metrics[0]))\n",
    "    axs1[1,1].plot(x, metrics[0],markers[index]+'-',color=colors[index],label=model_name)\n",
    "    axs1[1,1].set_title('Accuracy results over '+str(folds)+' folds')\n",
    "    axs1[1,1].set_xlabel('Folds')\n",
    "    axs1[1,1].set_ylabel('Accuracy')\n",
    "\n",
    "  axs1[1,1].legend(loc='upper center', bbox_to_anchor=(1.3, 0.75), fancybox=True, shadow=True)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure()\n",
    "  x = np.arange(4)\n",
    "  for index,model_name in enumerate(results[folds].keys()):\n",
    "    avg_metrics = []\n",
    "    for metric in results[folds][model_name][1]:\n",
    "      avg_metrics.append(np.mean(metric))\n",
    "    plt.plot(x,avg_metrics,markers[index]+'-',color=colors[index],label=model_name)\n",
    "\n",
    "  plt.title('Summary of '+str(folds)+'Folds Results')\n",
    "  plt.xlabel('Different Metrics')\n",
    "  plt.ylabel('Average of Folds results')\n",
    "  plt.xticks(x,['Accuracy', 'Precision Macro', 'Recall Macro', 'F1 Macro'], rotation=0.45)\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a344834-5502-404c-a3d0-9df06f0b21c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cluepoints)",
   "language": "python",
   "name": "cluepoints"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
